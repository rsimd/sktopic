{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")\n",
    "    \n",
    "import torch \n",
    "from skorch import callbacks\n",
    "import sktopic\n",
    "from sktopic.utils import manual_seed\n",
    "#from sktopic.models.base import ELBO\n",
    "#from sktopic.trainers import Trainer\n",
    "\n",
    "from sktopic.models import GaussianStickBreakingModel\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from octis.evaluation_metrics.coherence_metrics import WECoherencePairwise\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "\n",
    "import optuna\n",
    "from sktopic.callbacks import TopicQualityScoring, WECoherenceScoring\n",
    "from sktopic.metrics.npmi import NormalizedPointwiseMutualInformation as NPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using: SEED=950331\n"
     ]
    }
   ],
   "source": [
    "#from sktopic import utils\n",
    "from sktopic.datasets import fetch_20NewsGroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "manual_seed()\n",
    "dataset = fetch_20NewsGroups()\n",
    "X_tr,X_te = train_test_split(dataset[\"X\"])\n",
    "id2word = dataset[\"id2word\"]\n",
    "coherence = WECoherencePairwise()\n",
    "diversity = TopicDiversity()\n",
    "coherence_npmi = NPMI(dataset[\"X\"], id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_seed()\n",
    "# corpus_train = fetch_20newsgroups(subset=\"train\", remove=[\"headers\",\"footers\",\"quotes\"])\n",
    "# corpus_test = fetch_20newsgroups(subset=\"test\", remove=[\"headers\",\"footers\",\"quotes\"])\n",
    "\n",
    "# def clean_text(sentence):\n",
    "#     # remove non alphabetic sequences\n",
    "#     pattern = re.compile(r'[^a-z]+')\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence = pattern.sub(' ', sentence).strip()\n",
    "#     # Tokenize\n",
    "#     word_list = word_tokenize(sentence)\n",
    "    \n",
    "#     # stop words\n",
    "#     stopwords_list = set(stopwords.words('english'))\n",
    "#     # puctuation\n",
    "#     punct = set(string.punctuation)\n",
    "    \n",
    "#     # remove stop words\n",
    "#     word_list = [word for word in word_list if word not in stopwords_list]\n",
    "#     # remove very small words, length < 3\n",
    "#     # they don't contribute any useful information\n",
    "#     word_list = [word for word in word_list if len(word) > 2]\n",
    "#     # remove punctuation\n",
    "#     word_list = [word for word in word_list if word not in punct]\n",
    "#     # remove number \n",
    "#     word_list = [word for word in word_list if not word.isdigit()]\n",
    "#     # lemmatize\n",
    "#     lemma = WordNetLemmatizer()\n",
    "#     word_list = [lemma.lemmatize(word) for word in word_list]\n",
    "\n",
    "#     # remove stop words\n",
    "#     word_list = [word for word in word_list if word not in stopwords_list]\n",
    "#     # list to sentence\n",
    "#     sentence = ' '.join(word_list)\n",
    "#     return sentence\n",
    "\n",
    "# df_train = pd.DataFrame({'News': corpus_train.data,\n",
    "#                        'Target': corpus_train.target})\n",
    "# df_train['News'] = df_train['News'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# df_test = pd.DataFrame({'News': corpus_test.data,\n",
    "#                        'Target': corpus_test.target})\n",
    "# df_test['News'] = df_test['News'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# vectorizer = CountVectorizer(dtype=np.float32,lowercase=True, max_features=2000, max_df=0.5, min_df=10,stop_words=\"english\")\n",
    "# X_tr = vectorizer.fit_transform(df_train['News'].to_list())\n",
    "# X_te = vectorizer.transform(df_test['News'].to_list())\n",
    "\n",
    "# id2word = {k:v for k,v in enumerate(vectorizer.get_feature_names())}\n",
    "\n",
    "# mask_tr = np.array(X_tr.sum(1) > 0).flatten()\n",
    "# mask_te = np.array(X_te.sum(1) > 0).flatten()\n",
    "# X_tr = X_tr[mask_tr]\n",
    "# X_te = X_te[mask_te]\n",
    "\n",
    "# print(f\"{X_tr.shape=}\")\n",
    "# print(f\"{X_te.shape=}\")\n",
    "\n",
    "# #coherence = WECoherencePairwise()\n",
    "# diversity = TopicDiversity()\n",
    "# coherence_npmi = NPMI(X_tr, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using: SEED=950331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    train_ppl    valid_loss    valid_ppl    wetc_pw      lr     dur\n",
      "-------  ------------  -----------  ------------  -----------  ---------  ------  ------\n",
      "      1     \u001b[36m3809.7927\u001b[0m    \u001b[32m1488.4971\u001b[0m     \u001b[35m2345.2207\u001b[0m    \u001b[31m1159.8865\u001b[0m     \u001b[94m0.0887\u001b[0m  0.0500  0.8452\n",
      "      2     \u001b[36m1857.0911\u001b[0m    \u001b[32m1181.4513\u001b[0m      \u001b[35m728.4693\u001b[0m    \u001b[31m1088.0944\u001b[0m     \u001b[94m0.1029\u001b[0m  0.0488  0.8595\n",
      "      3      \u001b[36m967.9099\u001b[0m    \u001b[32m1127.3795\u001b[0m      \u001b[35m531.0765\u001b[0m    \u001b[31m1078.3788\u001b[0m     \u001b[94m0.1138\u001b[0m  0.0452  0.8666\n",
      "      4      \u001b[36m560.6776\u001b[0m    \u001b[32m1108.0131\u001b[0m      \u001b[35m403.6926\u001b[0m    \u001b[31m1075.1239\u001b[0m     0.1072  0.0397  0.8662\n",
      "      5      \u001b[36m415.8797\u001b[0m    \u001b[32m1098.7426\u001b[0m      \u001b[35m364.6209\u001b[0m    \u001b[31m1073.7463\u001b[0m     0.0990  0.0327  0.8547\n",
      "      6      \u001b[36m364.9222\u001b[0m    \u001b[32m1095.9938\u001b[0m      \u001b[35m347.7520\u001b[0m    \u001b[31m1072.9577\u001b[0m     0.0965  0.0250  0.8541\n",
      "      7      \u001b[36m346.9776\u001b[0m    \u001b[32m1093.0996\u001b[0m      \u001b[35m342.7735\u001b[0m    \u001b[31m1072.6297\u001b[0m     0.0998  0.0173  0.8513\n",
      "      8      \u001b[36m341.9746\u001b[0m    1093.3179      \u001b[35m340.7814\u001b[0m    \u001b[31m1072.5045\u001b[0m     0.0928  0.0103  0.8585\n",
      "      9      \u001b[36m339.8008\u001b[0m    \u001b[32m1092.4446\u001b[0m      \u001b[35m340.2730\u001b[0m    \u001b[31m1072.4560\u001b[0m     0.0923  0.0048  0.8579\n",
      "     10      \u001b[36m339.2176\u001b[0m    \u001b[32m1091.6437\u001b[0m      \u001b[35m340.1630\u001b[0m    \u001b[31m1072.4470\u001b[0m     0.0934  0.0012  0.8587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2250/2250.0 [00:04<00:00, 523.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tq=0.10285870801610035,wetc=0.092073,td=0.738,npmi=0.1393749431112471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "manual_seed()\n",
    "V = X_tr.shape[1]\n",
    "K = 20\n",
    "\n",
    "from torch import nn \n",
    "from sparsemax import Sparsemax\n",
    "from typing import Sequence, Optional\n",
    "from sktopic.components.mmd_loss import MMDLoss\n",
    "from sktopic.models import ProductOfExpertsLatentDirichletAllocation as ProdLDA\n",
    "import sktopic.models as mm \n",
    "\n",
    "optimizer_cls = torch.optim.Adam#ASGD\n",
    "\n",
    "model = mm.ProductOfExpertsLatentDirichletAllocation(\n",
    "    vocab_size=V, n_components=K,\n",
    "    optimizer=optimizer_cls,\n",
    "    batch_size=1000, lr=0.001,max_epochs=10,\n",
    "    device=\"cpu\",verbose=1,\n",
    "    callbacks=[\n",
    "        WECoherenceScoring(id2word, coherence_object=coherence),\n",
    "        callbacks.EarlyStopping(patience=5),\n",
    "        callbacks.LRScheduler(),\n",
    "        callbacks.GradientNormClipping(gradient_clip_value=1.0)\n",
    "        ],\n",
    "    #criterion=MMDLoss,\n",
    "    activation_hidden=\"Tanh\",\n",
    "    ) \n",
    "#mu, sigma = model.module.encoder(torch.from_numpy(X_te.toarray()))\n",
    "#(sigma == 0.0).sum()\n",
    "\n",
    "\n",
    "model.fit(X_tr)\n",
    "model_output = model.get_model_outputs(X_tr,id2word=id2word)\n",
    "\n",
    "try:\n",
    "    wetc = coherence.score(model_output)\n",
    "except:\n",
    "    wetc = 0.0\n",
    "td = diversity.score(model_output)\n",
    "npmi = coherence_npmi(model_output[\"topics\"]) \n",
    "tq = npmi * td\n",
    "\n",
    "print(f\"{tq=},{wetc=},{td=},{npmi=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tq=0.10514308448896476,wetc=0.08344203,td=0.886,npmi=0.11867165292208211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (100, 20)) of distribution Normal(loc: torch.Size([100, 20]), scale: torch.Size([100, 20])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_958398/451481227.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workdir/notebooks/../sktopic/trainers/vae.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, training)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mtopic_proportions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workdir/notebooks/../sktopic/components/nvdm.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, x, deterministic)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mposterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workdir/notebooks/../sktopic/distributions/distributions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mbase_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         super().__init__(base_dist, \n\u001b[1;32m     26\u001b[0m                         \u001b[0mSoftmaxTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/sktopic-L2WRRFYm-py3.9/lib/python3.9/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/sktopic-L2WRRFYm-py3.9/lib/python3.9/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     56\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (100, 20)) of distribution Normal(loc: torch.Size([100, 20]), scale: torch.Size([100, 20])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "model.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.utils import to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batches</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_batch_count</th>\n",
       "      <th>valid_batch_count</th>\n",
       "      <th>dur</th>\n",
       "      <th>valid_ppl</th>\n",
       "      <th>valid_ppl_best</th>\n",
       "      <th>train_ppl</th>\n",
       "      <th>train_ppl_best</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_loss_best</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_loss_best</th>\n",
       "      <th>event_lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'train_nll': 296.5760803222656, 'train_mmd':...</td>\n",
       "      <td>6</td>\n",
       "      <td>98</td>\n",
       "      <td>25</td>\n",
       "      <td>1.117952</td>\n",
       "      <td>1135.168092</td>\n",
       "      <td>True</td>\n",
       "      <td>1177.813497</td>\n",
       "      <td>True</td>\n",
       "      <td>331.976386</td>\n",
       "      <td>True</td>\n",
       "      <td>323.198382</td>\n",
       "      <td>True</td>\n",
       "      <td>0.025001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'train_nll': 297.05474853515625, 'train_mmd'...</td>\n",
       "      <td>7</td>\n",
       "      <td>98</td>\n",
       "      <td>25</td>\n",
       "      <td>1.073643</td>\n",
       "      <td>1129.446305</td>\n",
       "      <td>True</td>\n",
       "      <td>1169.838853</td>\n",
       "      <td>True</td>\n",
       "      <td>331.719853</td>\n",
       "      <td>True</td>\n",
       "      <td>322.990520</td>\n",
       "      <td>True</td>\n",
       "      <td>0.017275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'train_nll': 296.5000915527344, 'train_mmd':...</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>25</td>\n",
       "      <td>1.053726</td>\n",
       "      <td>1126.280294</td>\n",
       "      <td>True</td>\n",
       "      <td>1163.559574</td>\n",
       "      <td>True</td>\n",
       "      <td>331.364576</td>\n",
       "      <td>True</td>\n",
       "      <td>322.768260</td>\n",
       "      <td>True</td>\n",
       "      <td>0.010306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{'train_nll': 296.1229248046875, 'train_mmd':...</td>\n",
       "      <td>9</td>\n",
       "      <td>98</td>\n",
       "      <td>25</td>\n",
       "      <td>1.056663</td>\n",
       "      <td>1124.962236</td>\n",
       "      <td>True</td>\n",
       "      <td>1159.023742</td>\n",
       "      <td>True</td>\n",
       "      <td>331.145067</td>\n",
       "      <td>True</td>\n",
       "      <td>322.793836</td>\n",
       "      <td>False</td>\n",
       "      <td>0.004775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{'train_nll': 295.4415283203125, 'train_mmd':...</td>\n",
       "      <td>10</td>\n",
       "      <td>98</td>\n",
       "      <td>25</td>\n",
       "      <td>1.056243</td>\n",
       "      <td>1124.623689</td>\n",
       "      <td>True</td>\n",
       "      <td>1159.828309</td>\n",
       "      <td>False</td>\n",
       "      <td>331.220280</td>\n",
       "      <td>False</td>\n",
       "      <td>322.803825</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             batches  epoch  \\\n",
       "5  [{'train_nll': 296.5760803222656, 'train_mmd':...      6   \n",
       "6  [{'train_nll': 297.05474853515625, 'train_mmd'...      7   \n",
       "7  [{'train_nll': 296.5000915527344, 'train_mmd':...      8   \n",
       "8  [{'train_nll': 296.1229248046875, 'train_mmd':...      9   \n",
       "9  [{'train_nll': 295.4415283203125, 'train_mmd':...     10   \n",
       "\n",
       "   train_batch_count  valid_batch_count       dur    valid_ppl  \\\n",
       "5                 98                 25  1.117952  1135.168092   \n",
       "6                 98                 25  1.073643  1129.446305   \n",
       "7                 98                 25  1.053726  1126.280294   \n",
       "8                 98                 25  1.056663  1124.962236   \n",
       "9                 98                 25  1.056243  1124.623689   \n",
       "\n",
       "   valid_ppl_best    train_ppl  train_ppl_best  train_loss  train_loss_best  \\\n",
       "5            True  1177.813497            True  331.976386             True   \n",
       "6            True  1169.838853            True  331.719853             True   \n",
       "7            True  1163.559574            True  331.364576             True   \n",
       "8            True  1159.023742            True  331.145067             True   \n",
       "9            True  1159.828309           False  331.220280            False   \n",
       "\n",
       "   valid_loss  valid_loss_best  event_lr  \n",
       "5  323.198382             True  0.025001  \n",
       "6  322.990520             True  0.017275  \n",
       "7  322.768260             True  0.010306  \n",
       "8  322.793836            False  0.004775  \n",
       "9  322.803825            False  0.001225  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.history_).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic_0</th>\n",
       "      <td>afford</td>\n",
       "      <td>sin</td>\n",
       "      <td>combine</td>\n",
       "      <td>trial</td>\n",
       "      <td>trade</td>\n",
       "      <td>universe</td>\n",
       "      <td>talk</td>\n",
       "      <td>science</td>\n",
       "      <td>critical</td>\n",
       "      <td>punishment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_1</th>\n",
       "      <td>sleep</td>\n",
       "      <td>set</td>\n",
       "      <td>significantly</td>\n",
       "      <td>alive</td>\n",
       "      <td>hole</td>\n",
       "      <td>politic</td>\n",
       "      <td>affect</td>\n",
       "      <td>decent</td>\n",
       "      <td>insist</td>\n",
       "      <td>flow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_2</th>\n",
       "      <td>side</td>\n",
       "      <td>option</td>\n",
       "      <td>graphic</td>\n",
       "      <td>style</td>\n",
       "      <td>heat</td>\n",
       "      <td>doesn</td>\n",
       "      <td>parallel</td>\n",
       "      <td>accident</td>\n",
       "      <td>deserve</td>\n",
       "      <td>minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_3</th>\n",
       "      <td>simply</td>\n",
       "      <td>leg</td>\n",
       "      <td>god</td>\n",
       "      <td>kick</td>\n",
       "      <td>staff</td>\n",
       "      <td>surrender</td>\n",
       "      <td>ability</td>\n",
       "      <td>guy</td>\n",
       "      <td>ride</td>\n",
       "      <td>request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_4</th>\n",
       "      <td>wheel</td>\n",
       "      <td>brake</td>\n",
       "      <td>yesterday</td>\n",
       "      <td>public</td>\n",
       "      <td>eat</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>guess</td>\n",
       "      <td>box</td>\n",
       "      <td>campaign</td>\n",
       "      <td>doesn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_5</th>\n",
       "      <td>location</td>\n",
       "      <td>north</td>\n",
       "      <td>primary</td>\n",
       "      <td>examine</td>\n",
       "      <td>principle</td>\n",
       "      <td>truck</td>\n",
       "      <td>debate</td>\n",
       "      <td>student</td>\n",
       "      <td>couple</td>\n",
       "      <td>suspect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_6</th>\n",
       "      <td>decide</td>\n",
       "      <td>main</td>\n",
       "      <td>format</td>\n",
       "      <td>fine</td>\n",
       "      <td>draft</td>\n",
       "      <td>previously</td>\n",
       "      <td>oppose</td>\n",
       "      <td>argue</td>\n",
       "      <td>peace</td>\n",
       "      <td>automatically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_7</th>\n",
       "      <td>threaten</td>\n",
       "      <td>movie</td>\n",
       "      <td>case</td>\n",
       "      <td>exercise</td>\n",
       "      <td>arm</td>\n",
       "      <td>church</td>\n",
       "      <td>implementation</td>\n",
       "      <td>suggestion</td>\n",
       "      <td>intent</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_8</th>\n",
       "      <td>apply</td>\n",
       "      <td>street</td>\n",
       "      <td>insert</td>\n",
       "      <td>component</td>\n",
       "      <td>role</td>\n",
       "      <td>possibly</td>\n",
       "      <td>recent</td>\n",
       "      <td>instruction</td>\n",
       "      <td>bother</td>\n",
       "      <td>restrict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_9</th>\n",
       "      <td>surface</td>\n",
       "      <td>email</td>\n",
       "      <td>brain</td>\n",
       "      <td>pop</td>\n",
       "      <td>manufacture</td>\n",
       "      <td>refer</td>\n",
       "      <td>historical</td>\n",
       "      <td>reverse</td>\n",
       "      <td>observe</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_10</th>\n",
       "      <td>widely</td>\n",
       "      <td>mouse</td>\n",
       "      <td>original</td>\n",
       "      <td>advanced</td>\n",
       "      <td>capable</td>\n",
       "      <td>european</td>\n",
       "      <td>publication</td>\n",
       "      <td>peace</td>\n",
       "      <td>consistent</td>\n",
       "      <td>contribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_11</th>\n",
       "      <td>exist</td>\n",
       "      <td>review</td>\n",
       "      <td>son</td>\n",
       "      <td>professional</td>\n",
       "      <td>motherboard</td>\n",
       "      <td>reaction</td>\n",
       "      <td>meg</td>\n",
       "      <td>modify</td>\n",
       "      <td>innocent</td>\n",
       "      <td>slow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_12</th>\n",
       "      <td>king</td>\n",
       "      <td>energy</td>\n",
       "      <td>original</td>\n",
       "      <td>draw</td>\n",
       "      <td>sit</td>\n",
       "      <td>pitch</td>\n",
       "      <td>generate</td>\n",
       "      <td>cop</td>\n",
       "      <td>specifically</td>\n",
       "      <td>half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_13</th>\n",
       "      <td>catch</td>\n",
       "      <td>conduct</td>\n",
       "      <td>result</td>\n",
       "      <td>attention</td>\n",
       "      <td>mount</td>\n",
       "      <td>scan</td>\n",
       "      <td>waste</td>\n",
       "      <td>vehicle</td>\n",
       "      <td>poor</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_14</th>\n",
       "      <td>originally</td>\n",
       "      <td>nation</td>\n",
       "      <td>vendor</td>\n",
       "      <td>death</td>\n",
       "      <td>federal</td>\n",
       "      <td>border</td>\n",
       "      <td>size</td>\n",
       "      <td>major</td>\n",
       "      <td>week</td>\n",
       "      <td>wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_15</th>\n",
       "      <td>clear</td>\n",
       "      <td>check</td>\n",
       "      <td>talk</td>\n",
       "      <td>manage</td>\n",
       "      <td>modify</td>\n",
       "      <td>common</td>\n",
       "      <td>deep</td>\n",
       "      <td>kick</td>\n",
       "      <td>thousand</td>\n",
       "      <td>examine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_16</th>\n",
       "      <td>clock</td>\n",
       "      <td>boot</td>\n",
       "      <td>universe</td>\n",
       "      <td>top</td>\n",
       "      <td>guide</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>satellite</td>\n",
       "      <td>harm</td>\n",
       "      <td>wing</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_17</th>\n",
       "      <td>taxis</td>\n",
       "      <td>pay</td>\n",
       "      <td>strongly</td>\n",
       "      <td>track</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>reduce</td>\n",
       "      <td>home</td>\n",
       "      <td>serve</td>\n",
       "      <td>rest</td>\n",
       "      <td>bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_18</th>\n",
       "      <td>university</td>\n",
       "      <td>crap</td>\n",
       "      <td>network</td>\n",
       "      <td>action</td>\n",
       "      <td>relevant</td>\n",
       "      <td>hand</td>\n",
       "      <td>variety</td>\n",
       "      <td>configuration</td>\n",
       "      <td>worry</td>\n",
       "      <td>image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic_19</th>\n",
       "      <td>site</td>\n",
       "      <td>bit</td>\n",
       "      <td>check</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>disagree</td>\n",
       "      <td>agree</td>\n",
       "      <td>lie</td>\n",
       "      <td>item</td>\n",
       "      <td>cut</td>\n",
       "      <td>separate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0        1              2             3            4  \\\n",
       "Topic_0       afford      sin        combine         trial        trade   \n",
       "Topic_1        sleep      set  significantly         alive         hole   \n",
       "Topic_2         side   option        graphic         style         heat   \n",
       "Topic_3       simply      leg            god          kick        staff   \n",
       "Topic_4        wheel    brake      yesterday        public          eat   \n",
       "Topic_5     location    north        primary       examine    principle   \n",
       "Topic_6       decide     main         format          fine        draft   \n",
       "Topic_7     threaten    movie           case      exercise          arm   \n",
       "Topic_8        apply   street         insert     component         role   \n",
       "Topic_9      surface    email          brain           pop  manufacture   \n",
       "Topic_10      widely    mouse       original      advanced      capable   \n",
       "Topic_11       exist   review            son  professional  motherboard   \n",
       "Topic_12        king   energy       original          draw          sit   \n",
       "Topic_13       catch  conduct         result     attention        mount   \n",
       "Topic_14  originally   nation         vendor         death      federal   \n",
       "Topic_15       clear    check           talk        manage       modify   \n",
       "Topic_16       clock     boot       universe           top        guide   \n",
       "Topic_17       taxis      pay       strongly         track  demonstrate   \n",
       "Topic_18  university     crap        network        action     relevant   \n",
       "Topic_19        site      bit          check     paragraph     disagree   \n",
       "\n",
       "                   5               6              7             8  \\\n",
       "Topic_0     universe            talk        science      critical   \n",
       "Topic_1      politic          affect         decent        insist   \n",
       "Topic_2        doesn        parallel       accident       deserve   \n",
       "Topic_3    surrender         ability            guy          ride   \n",
       "Topic_4   acceptable           guess            box      campaign   \n",
       "Topic_5        truck          debate        student        couple   \n",
       "Topic_6   previously          oppose          argue         peace   \n",
       "Topic_7       church  implementation     suggestion        intent   \n",
       "Topic_8     possibly          recent    instruction        bother   \n",
       "Topic_9        refer      historical        reverse       observe   \n",
       "Topic_10    european     publication          peace    consistent   \n",
       "Topic_11    reaction             meg         modify      innocent   \n",
       "Topic_12       pitch        generate            cop  specifically   \n",
       "Topic_13        scan           waste        vehicle          poor   \n",
       "Topic_14      border            size          major          week   \n",
       "Topic_15      common            deep           kick      thousand   \n",
       "Topic_16    keyboard       satellite           harm          wing   \n",
       "Topic_17      reduce            home          serve          rest   \n",
       "Topic_18        hand         variety  configuration         worry   \n",
       "Topic_19       agree             lie           item           cut   \n",
       "\n",
       "                      9  \n",
       "Topic_0      punishment  \n",
       "Topic_1            flow  \n",
       "Topic_2         minimum  \n",
       "Topic_3         request  \n",
       "Topic_4           doesn  \n",
       "Topic_5         suspect  \n",
       "Topic_6   automatically  \n",
       "Topic_7          method  \n",
       "Topic_8        restrict  \n",
       "Topic_9        innocent  \n",
       "Topic_10     contribute  \n",
       "Topic_11           slow  \n",
       "Topic_12           half  \n",
       "Topic_13          agree  \n",
       "Topic_14           wear  \n",
       "Topic_15        examine  \n",
       "Topic_16          house  \n",
       "Topic_17           bomb  \n",
       "Topic_18          image  \n",
       "Topic_19       separate  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = model.get_topic_top_words(id2word).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0>> 余裕、罪、コンバイン、トライアル、貿易、宇宙、話、科学、批判的、罰\n",
      "1>> 睡眠、設定、大幅に、生きている、穴、政治的、影響、まともな、主張、流れ\n",
      "2>> 側面、オプション、グラフィック、スタイル、熱、しない、並列、事故、値、最小\n",
      "3>> 単に足、神、キック、スタッフ、降伏、能力、男、乗る、要求\n",
      "4>> ホイール、ブレーキ、昨日、一般、食べる、許容できる、推測、箱、キャンペーン、行い\n",
      "5>> 場所、北、一次、検査、原理、トラック、議論、学生、カップル、疑わしい\n",
      "6>> 決定、メイン、フォーマット、ファイン、ドラフト、以前は反対、議論、平和、自動的に\n",
      "7>> 脅迫、映画、ケース、運動、腕、教会、実装、提案、意図、方法\n",
      "8>> 適用、ストリート、インサート、コンポーネント、役割、おそらく最近の、命令、やはり、制限\n",
      "9>> 表面、電子メール、脳、ポップ、製造、参照、歴史的、逆、観察、無邪気\n",
      "10>> 広く、マウス、オリジナル、高度、有能、ヨーロッパ、出版、平和、一貫性、貢献\n",
      "11>> 存在、レビュー、息子、プロの、マザーボード、反応、MEG、修正、無邪気、遅い\n",
      "12>> 王、エネルギー、オリジナル、描画、座り、ピッチ、生成、警官、具体的には半分\n",
      "13>> キャッチ、行動、結果、注意、マウント、スキャン、廃棄物、車両、貧弱な、同意する\n",
      "14>> もともと、国、ベンダー、死、連邦、国境、サイズ、メジャー、週、wear\n",
      "15>> クリア、チェック、話、管理、変更、共通、深い、キック、千、検査\n",
      "16>> 時計、ブーツ、ユニバース、トップ、ガイド、キーボード、衛星、危害、翼、家\n",
      "17>> タクシー、支払い、強く、追跡、実証、軽減、家、サービス、休息、爆弾\n",
      "18>> 大学、クラップ、ネットワーク、行動、関連、手、バラエティ、構成、心配、イメージ\n",
      "19>> サイト、ビット、チェック、段落、同意、賛成、うそ、アイテム、カット、分離\n"
     ]
    }
   ],
   "source": [
    "trans = Translator()\n",
    "for ix, line in enumerate(df.to_numpy().tolist()):\n",
    "    line = \", \".join(line)\n",
    "    ans = trans.translate(line, src=\"en\",dest=\"ja\")\n",
    "    print(f\"{ix}>>\",ans.text)\n",
    "    #sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10783f88d19cbf6629da312ee526446d1cda014bbfa2009b25fff1cd3883c755"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sktopic-L2WRRFYm-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
